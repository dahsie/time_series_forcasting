{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b46c-89e5-43ca-a2ac-8117b80600a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90948c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c34929-685f-46cf-8969-845ef88c61ce",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50dc4f71-ac8e-461c-ae2d-140aefd2e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"This class allow creating a model consist of LSTM and Dense layers in order to forcast time series\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size,output_size,hidden_size,num_layers,device=\"cuda\",dropout=None):\n",
    "        \"\"\"Constructor of our time serie forcasting model\n",
    "            Args:\n",
    "                input_size  : The nummber of feature of an input. In our case it is generaly equal to 1\n",
    "                output_size : The number of output values of the model. In our case it generaly equal to 1\n",
    "                hidden_size : The number of features in the hidden state h\n",
    "                num_layers  :  Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs \n",
    "\n",
    "            Return : Nothing\n",
    "        \"\"\"\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.device= torch.device(\"cuda\") if device==\"cuda\" else torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "        self.lstm= nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,dropout=dropout) if dropout else nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fc=nn.Linear(in_features=hidden_size,out_features=output_size)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"Make the forword pass\n",
    "            Args:\n",
    "                inputs : The inputs which should be provided to model for the forward pass\n",
    "\n",
    "            Return : The result of the forward pass \n",
    "        \"\"\"\n",
    "        batch_size=inputs.size(0)\n",
    "        h=torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
    "        c=torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
    "        x,_=self.lstm(inputs,(h,c))\n",
    "        return self.fc(x[:,-1,:])\n",
    "\n",
    "    def train_one_epoch(self,train_loader,optimizer,loss_fn):\n",
    "        \n",
    "        \"\"\"The function is use to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer      : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function  : The loss function to use during the model training. CrossEntropyLoss and MSELoss are very often use for respectively\n",
    "                                classification and regression problem\n",
    "                train_loader   : A torch DataLoader which is use to iterate through the data during train step\n",
    "\n",
    "            Return : The last loss value on the train_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad() # Putting gradient to zero in order to accumulate upcoming gradient\n",
    "\n",
    "            #print(batch.size)\n",
    "            data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "           # data,target=data.to(self.device),target.to(self.device)\n",
    "          \n",
    "            outputs=self(data) # Making prediction\n",
    "\n",
    "            loss=loss_fn(outputs.to(self.device),target) # Computing the loss\n",
    "            loss.backward() # Computing the gradient\n",
    "\n",
    "            running_loss+=loss.item() # Accumulation loss through one epoch\n",
    "\n",
    "            optimizer.step() # Updating weights\n",
    "\n",
    "            if batch_index%100==99:\n",
    "                last_loss=running_loss/100 # Computing loss per batch\n",
    "                # print(\"batch {} loss {}\".format(batch_index+1,last_loss))\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    def validation_one_epoch(self,validation_loader,loss_fn):\n",
    "        \n",
    "        \"\"\"The function is used to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer       : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function   : The loss function to use during the model validation. Same as the training one\n",
    "                val_loader      : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : The last loss value on the val_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        avg_vloss = 0.\n",
    "        running_loss=0.\n",
    "        with torch.no_grad() :\n",
    "            for batch in validation_loader:\n",
    "    \n",
    "                data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "                outputs=self(data) # Making prediction\n",
    "\n",
    "                loss=loss_fn(outputs,target) # Computing the loss\n",
    "                \n",
    "                running_loss+=loss.item() # Accumulation loss through one epoch\n",
    "                \n",
    "        avg_vloss=running_loss/len(validation_loader)\n",
    "                   \n",
    "        return avg_vloss\n",
    "\n",
    "\n",
    "    def predict(self,test_loader):\n",
    "\n",
    "        \"\"\"The function is used to train one epoch of the model\n",
    "            Args: \n",
    "                test_loader      : A torch DataLoader which is use to iterate through the data during prediction step\n",
    "\n",
    "            Return : Predicted and actual value of the test_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        out=[]\n",
    "        targets=[]\n",
    "        with torch.no_grad() :\n",
    "            for batch in tqdm(test_loader):\n",
    "                data,target=batch[0].to('cuda'),batch[1]\n",
    "                outputs=self(data) # Making prediction\n",
    "    \n",
    "                out.append(outputs[0].cpu())\n",
    "                targets.append(target[0])\n",
    "                \n",
    "        return np.array(out), np.array(targets)\n",
    "\n",
    "    def evaluation(self,y_test,predictions,metric=\"mse\"):\n",
    "        # Evaluate the model\n",
    "        if metric==\"mse\" :print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, predictions))\n",
    "        elif metric ==\"mae\" :print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, predictions))\n",
    "        \n",
    "\n",
    "    def fit(self,train_loader,validation_loader,optimizer,loss_fn,epochs=100,device=\"cuda\"):\n",
    "        \"\"\"The function is used to train the model through many epochs\n",
    "            Args: \n",
    "                optimizer             : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                train_loader          : A torch DataLoader which is use to iterate through the data during train step\n",
    "                loss_function         : The loss function to use during the model validation. Same as the training one\n",
    "                validataion_loader    : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : No return\n",
    "                \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        best_avg_vloss=1_000_000_000\n",
    "        for epoch in range(epochs):\n",
    "            # print(\"\\n\")\n",
    "            print(\"EPOCHS : {}\".format(epoch +1))\n",
    "            self.train(True) # Activate training mode\n",
    "            avg_loss=self.train_one_epoch(train_loader,optimizer,loss_fn) # training the model on an epoch\n",
    "            \n",
    "            self.eval() # Setting the evaluation mode so that no gradient will be compute, that will save forward pass time\n",
    "            avg_vloss=self.validation_one_epoch(validation_loader,loss_fn) # Evalution the model after one epoch\n",
    "            print(f\"Loss {avg_loss} vs Validation_loss  {avg_vloss}\")\n",
    "\n",
    "            if avg_vloss < best_avg_vloss :\n",
    "                best_avg_vloss=avg_vloss\n",
    "                model_path='/home/dah/timeSeries/time_series_forcasting/models/model_{}'.format(datetime.now().strftime('%Y%m%d_%H%')) \n",
    "                torch.save(self.cpu().state_dict(),model_path)\n",
    "                self.to(self.device)\n",
    "                self.to(self.device)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea768e-6164-40f6-a271-a7e1fdf9b492",
   "metadata": {},
   "source": [
    "# Custom dataset for training LSTM-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26551091-b5c8-4311-9b27-0910ca1893f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerieDataset(Dataset):\n",
    "    def __init__(self,data,time_step,batch_size,transform=None):\n",
    "        super(TimeSerieDataset,self).__init__()\n",
    "        self.time_step=time_step\n",
    "        self.transform=transform\n",
    "        self.data,self.target=self._sequence_generation(data,time_step)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if index >=len(self.data) :\n",
    "            raise \"Out of range\"\n",
    "        # return self.data[index:min(index+self.batch_size,len(self.data))],self.target[index:min(index+self.batch_size,len(self.data))]\n",
    "        return self.data[index],self.target[index]\n",
    "\n",
    "    def _sequence_generation(self,data,time_step):\n",
    "        X,y=list(),list()\n",
    "        data=self.transform.fit_transform(data) if self.transform else data\n",
    "        for i in range(len(data) - time_step):\n",
    "            X.append(data[i:i+time_step,])\n",
    "            y.append(data[i+time_step,])\n",
    "            \n",
    "        return torch.tensor(X,dtype=torch.float32).unsqueeze(-1),torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40915ed-f07c-436f-8fbe-14e9e7292442",
   "metadata": {},
   "source": [
    "# The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ff3ada2-1580-4ab6-9d3d-4fd8377db242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    input_size=1\n",
    "    output_size=1\n",
    "    hidden_size=512\n",
    "    num_layers=1\n",
    "    model=LSTM(input_size,1,hidden_size,num_layers)\n",
    "    # print(f\"mode.device {model.device}\")\n",
    "    # print(f\"model {model}\")\n",
    "\n",
    "    humidity = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/humidity.csv\")\n",
    "    temp = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/temperature.csv\")\n",
    "    pressure = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/pressure.csv\")\n",
    "    \n",
    "    ville='Montreal'\n",
    "    humidity_M = humidity[['datetime',ville]]\n",
    "    temp_M = temp[['datetime',ville]]\n",
    "    pressure_M = pressure[['datetime',ville]]\n",
    "\n",
    "    \n",
    "\n",
    "    humidity_M.interpolate(inplace=True)\n",
    "    humidity_M.dropna(inplace=True)\n",
    "    nb_samples = int(len(humidity_M)*0.90)\n",
    "    \n",
    "    humidity_train = np.array(humidity_M[ville][:nb_samples]).ravel()\n",
    "    humidity_test = np.array(humidity_M[ville][nb_samples:]).ravel()\n",
    "    \n",
    "    temp_train = np.array(temp_M[ville][:nb_samples]).ravel()\n",
    "    temp_test = np.array(temp_M[ville][nb_samples:]).ravel()\n",
    "    \n",
    "    pressure_train = np.array(pressure_M[ville][:nb_samples]).ravel()\n",
    "    pressure_test = np.array(pressure_M[ville][nb_samples:]).ravel()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # temp_M.interpolate(inplace=True)\n",
    "    # temp_M.dropna(inplace=True)\n",
    "    \n",
    "    # pressure_M.interpolate(inplace=True)\n",
    "    # pressure_M.dropna(inplace=True)\n",
    "\n",
    "    time_step=8\n",
    "\n",
    "    train_batch_size,test_batch_size=10,10\n",
    "    xtrain=TimeSerieDataset(humidity_train,time_step,batch_size=train_batch_size)\n",
    "    xtest=TimeSerieDataset(humidity_test,time_step,batch_size=test_batch_size)\n",
    "    train_loader=DataLoader(dataset=xtrain,batch_size=train_batch_size,num_workers=15)\n",
    "    test_loader=DataLoader(dataset=xtest,batch_size=test_batch_size,num_workers=15)\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(),lr=1e-5)\n",
    "    loss_fn=nn.MSELoss()\n",
    "\n",
    "    model.fit(train_loader=train_loader,validation_loader=test_loader,optimizer=optimizer,loss_fn=loss_fn,epochs=200)\n",
    "\n",
    "    #plt.plot(humidity_test)\n",
    "    out,targets=model.predict(test_loader)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(22, 4))\n",
    "    plt.plot(out,label=\"Predicted\")\n",
    "    plt.plot(targets,label=\"Actual\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.evaluation(out,targets,\"mse\")\n",
    "    model.evaluation(out,targets,\"mae\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea997f0-a93f-482b-be69-26caaaac58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 1\n",
      "Loss 127665.84281532287 vs Validation_loss  2040.3065556045128\n",
      "EPOCHS : 2\n",
      "Loss 56832.81813946724 vs Validation_loss  891.4665563317527\n",
      "EPOCHS : 3\n",
      "Loss 24414.468852107526 vs Validation_loss  381.48302214230057\n",
      "EPOCHS : 4\n",
      "Loss 11182.46591404438 vs Validation_loss  212.00044889576668\n",
      "EPOCHS : 5\n",
      "Loss 6668.312363912762 vs Validation_loss  163.24309845308287\n",
      "EPOCHS : 6\n",
      "Loss 5293.258579738271 vs Validation_loss  149.84749075691258\n",
      "EPOCHS : 7\n",
      "Loss 4883.262364026457 vs Validation_loss  145.89562977731757\n",
      "EPOCHS : 8\n",
      "Loss 4745.866542085335 vs Validation_loss  144.4450462455243\n",
      "EPOCHS : 9\n",
      "Loss 4687.862741957801 vs Validation_loss  143.7610561826588\n",
      "EPOCHS : 10\n",
      "Loss 4658.2901331845205 vs Validation_loss  143.2736669124755\n",
      "EPOCHS : 11\n",
      "Loss 4641.18107641507 vs Validation_loss  142.90056683742895\n",
      "EPOCHS : 12\n",
      "Loss 4630.518343261909 vs Validation_loss  142.5753012958881\n",
      "EPOCHS : 13\n",
      "Loss 4623.504223539382 vs Validation_loss  142.23715270304046\n",
      "EPOCHS : 14\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058783-9d81-4832-831f-5df05b70daf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
