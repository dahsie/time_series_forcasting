{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b46c-89e5-43ca-a2ac-8117b80600a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90948c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c34929-685f-46cf-8969-845ef88c61ce",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50dc4f71-ac8e-461c-ae2d-140aefd2e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"This class allow creating a model consist of LSTM and Dense layers in order to forcast time series\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size,output_size,hidden_size,num_layers,device=\"cuda\",dropout=None):\n",
    "        \"\"\"Constructor of our time serie forcasting model\n",
    "            Args:\n",
    "                input_size  : The nummber of feature of an input. In our case it is generaly equal to 1\n",
    "                output_size : The number of output values of the model. In our case it generaly equal to 1\n",
    "                hidden_size : The number of features in the hidden state h\n",
    "                num_layers  :  Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs \n",
    "\n",
    "            Return : Nothing\n",
    "        \"\"\"\n",
    "        super(LSTM,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.device= torch.device(\"cuda\") if device==\"cuda\" else torch.device(\"cpu\")\n",
    "        \n",
    "        \n",
    "        self.lstm= nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,dropout=dropout) if dropout else nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fc=nn.Linear(in_features=hidden_size,out_features=output_size)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"Make the forword pass\n",
    "            Args:\n",
    "                inputs : The inputs which should be provided to model for the forward pass\n",
    "\n",
    "            Return : The result of the forward pass \n",
    "        \"\"\"\n",
    "        batch_size=inputs.size(0)\n",
    "        h=torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
    "        c=torch.zeros(self.num_layers,batch_size,self.hidden_size).to(self.device)\n",
    "        x,_=self.lstm(inputs,(h,c))\n",
    "        return self.fc(x[:,-1,:])\n",
    "\n",
    "    def train_one_epoch(self,train_loader,optimizer,loss_fn):\n",
    "        \n",
    "        \"\"\"The function is use to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer      : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function  : The loss function to use during the model training. CrossEntropyLoss and MSELoss are very often use for respectively\n",
    "                                classification and regression problem\n",
    "                train_loader   : A torch DataLoader which is use to iterate through the data during train step\n",
    "\n",
    "            Return : The last loss value on the train_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad() # Putting gradient to zero in order to accumulate upcoming gradient\n",
    "\n",
    "            #print(batch.size)\n",
    "            data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "           # data,target=data.to(self.device),target.to(self.device)\n",
    "          \n",
    "            outputs=self(data) # Making prediction\n",
    "\n",
    "            loss=loss_fn(outputs.to(self.device),target) # Computing the loss\n",
    "            loss.backward() # Computing the gradient\n",
    "\n",
    "            running_loss+=loss.item() # Accumulation loss through one epoch\n",
    "\n",
    "            optimizer.step() # Updating weights\n",
    "\n",
    "            if batch_index%100==99:\n",
    "                last_loss=running_loss/100 # Computing loss per batch\n",
    "                # print(\"batch {} loss {}\".format(batch_index+1,last_loss))\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    def validation_one_epoch(self,validation_loader,loss_fn):\n",
    "        \n",
    "        \"\"\"The function is used to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer       : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function   : The loss function to use during the model validation. Same as the training one\n",
    "                val_loader      : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : The last loss value on the val_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        avg_vloss = 0.\n",
    "        running_loss=0.\n",
    "        with torch.no_grad() :\n",
    "            for batch in validation_loader:\n",
    "    \n",
    "                data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "                outputs=self(data) # Making prediction\n",
    "\n",
    "                loss=loss_fn(outputs,target) # Computing the loss\n",
    "                \n",
    "                running_loss+=loss.item() # Accumulation loss through one epoch\n",
    "                \n",
    "        avg_vloss=running_loss/len(validation_loader)\n",
    "                   \n",
    "        return avg_vloss\n",
    "\n",
    "\n",
    "    def predict(self,test_loader):\n",
    "\n",
    "        \"\"\"The function is used to train one epoch of the model\n",
    "            Args: \n",
    "                test_loader      : A torch DataLoader which is use to iterate through the data during prediction step\n",
    "\n",
    "            Return : Predicted and actual value of the test_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        out=[]\n",
    "        targets=[]\n",
    "        with torch.no_grad() :\n",
    "            for batch in tqdm(test_loader):\n",
    "                data,target=batch[0].to('cuda'),batch[1]\n",
    "                outputs=self(data) # Making prediction\n",
    "    \n",
    "                out.append(outputs[0].cpu())\n",
    "                targets.append(target[0])\n",
    "                \n",
    "        return np.array(out), np.array(targets)\n",
    "\n",
    "    def evaluation(self,y_test,predictions,metric=\"mse\"):\n",
    "        # Evaluate the model\n",
    "        if metric==\"mse\" :print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, predictions))\n",
    "        elif metric ==\"mae\" :print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, predictions))\n",
    "        \n",
    "\n",
    "    def fit(self,train_loader,validation_loader,optimizer,loss_fn,epochs=100,device=\"cuda\"):\n",
    "        \"\"\"The function is used to train the model through many epochs\n",
    "            Args: \n",
    "                optimizer             : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                train_loader          : A torch DataLoader which is use to iterate through the data during train step\n",
    "                loss_function         : The loss function to use during the model validation. Same as the training one\n",
    "                validataion_loader    : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : No return\n",
    "                \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        best_avg_vloss=1_000_000_000\n",
    "        for epoch in range(epochs):\n",
    "            # print(\"\\n\")\n",
    "            print(\"EPOCHS : {}\".format(epoch +1))\n",
    "            self.train(True) # Activate training mode\n",
    "            avg_loss=self.train_one_epoch(train_loader,optimizer,loss_fn) # training the model on an epoch\n",
    "            \n",
    "            self.eval() # Setting the evaluation mode so that no gradient will be compute, that will save forward pass time\n",
    "            avg_vloss=self.validation_one_epoch(validation_loader,loss_fn) # Evalution the model after one epoch\n",
    "            print(f\"Loss {avg_loss} vs Validation_loss  {avg_vloss}\")\n",
    "\n",
    "            if avg_vloss < best_avg_vloss :\n",
    "                best_avg_vloss=avg_vloss\n",
    "                model_path='/home/dah/timeSeries/time_series_forcasting/models/model_{}'.format(datetime.now().strftime('%Y%m%d_%H%')) \n",
    "                torch.save(self.cpu().state_dict(),model_path)\n",
    "                self.to(self.device)\n",
    "                self.to(self.device)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea768e-6164-40f6-a271-a7e1fdf9b492",
   "metadata": {},
   "source": [
    "# Custom dataset for training LSTM-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26551091-b5c8-4311-9b27-0910ca1893f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSerieDataset(Dataset):\n",
    "    def __init__(self,data,time_step,batch_size,transform=None):\n",
    "        super(TimeSerieDataset,self).__init__()\n",
    "        self.time_step=time_step\n",
    "        self.transform=transform\n",
    "        self.data,self.target=self._sequence_generation(data,time_step)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if index >=len(self.data) :\n",
    "            raise \"Out of range\"\n",
    "        # return self.data[index:min(index+self.batch_size,len(self.data))],self.target[index:min(index+self.batch_size,len(self.data))]\n",
    "        return self.data[index],self.target[index]\n",
    "\n",
    "    def _sequence_generation(self,data,time_step):\n",
    "        X,y=list(),list()\n",
    "        data=self.transform.fit_transform(data) if self.transform else data\n",
    "        for i in range(len(data) - time_step):\n",
    "            X.append(data[i:i+time_step,])\n",
    "            y.append(data[i+time_step,])\n",
    "            \n",
    "        return torch.tensor(X,dtype=torch.float32).unsqueeze(-1),torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40915ed-f07c-436f-8fbe-14e9e7292442",
   "metadata": {},
   "source": [
    "# The main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ff3ada2-1580-4ab6-9d3d-4fd8377db242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    input_size=1\n",
    "    output_size=1\n",
    "    hidden_size=512\n",
    "    num_layers=1\n",
    "    model=LSTM(input_size,1,hidden_size,num_layers)\n",
    "    # print(f\"mode.device {model.device}\")\n",
    "    # print(f\"model {model}\")\n",
    "\n",
    "    humidity = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/humidity.csv\")\n",
    "    temp = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/temperature.csv\")\n",
    "    pressure = pd.read_csv(\"/home/dah/deep_learning_isima/tp_reseaux recurrents/datasets/pressure.csv\")\n",
    "    \n",
    "    ville='Montreal'\n",
    "    humidity_M = humidity[['datetime',ville]]\n",
    "    temp_M = temp[['datetime',ville]]\n",
    "    pressure_M = pressure[['datetime',ville]]\n",
    "\n",
    "    \n",
    "\n",
    "    humidity_M.interpolate(inplace=True)\n",
    "    humidity_M.dropna(inplace=True)\n",
    "    nb_samples = int(len(humidity_M)*0.90)\n",
    "    \n",
    "    humidity_train = np.array(humidity_M[ville][:nb_samples]).ravel()\n",
    "    humidity_test = np.array(humidity_M[ville][nb_samples:]).ravel()\n",
    "    \n",
    "    temp_train = np.array(temp_M[ville][:nb_samples]).ravel()\n",
    "    temp_test = np.array(temp_M[ville][nb_samples:]).ravel()\n",
    "    \n",
    "    pressure_train = np.array(pressure_M[ville][:nb_samples]).ravel()\n",
    "    pressure_test = np.array(pressure_M[ville][nb_samples:]).ravel()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # temp_M.interpolate(inplace=True)\n",
    "    # temp_M.dropna(inplace=True)\n",
    "    \n",
    "    # pressure_M.interpolate(inplace=True)\n",
    "    # pressure_M.dropna(inplace=True)\n",
    "\n",
    "    time_step=8\n",
    "\n",
    "    train_batch_size,test_batch_size=10,10\n",
    "    xtrain=TimeSerieDataset(humidity_train,time_step,batch_size=train_batch_size)\n",
    "    xtest=TimeSerieDataset(humidity_test,time_step,batch_size=test_batch_size)\n",
    "    train_loader=DataLoader(dataset=xtrain,batch_size=train_batch_size,num_workers=15)\n",
    "    test_loader=DataLoader(dataset=xtest,batch_size=test_batch_size,num_workers=15)\n",
    "\n",
    "    optimizer=optim.Adam(model.parameters(),lr=1e-5)\n",
    "    loss_fn=nn.MSELoss()\n",
    "\n",
    "    model.fit(train_loader=train_loader,validation_loader=test_loader,optimizer=optimizer,loss_fn=loss_fn,epochs=200)\n",
    "\n",
    "    #plt.plot(humidity_test)\n",
    "    out,targets=model.predict(test_loader)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(22, 4))\n",
    "    plt.plot(out,label=\"Predicted\")\n",
    "    plt.plot(targets,label=\"Actual\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.evaluation(out,targets,\"mse\")\n",
    "    model.evaluation(out,targets,\"mae\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea997f0-a93f-482b-be69-26caaaac58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 1\n",
      "Loss 127665.84281532287 vs Validation_loss  2040.3065556045128\n",
      "EPOCHS : 2\n",
      "Loss 56832.81813946724 vs Validation_loss  891.4665563317527\n",
      "EPOCHS : 3\n",
      "Loss 24414.468852107526 vs Validation_loss  381.48302214230057\n",
      "EPOCHS : 4\n",
      "Loss 11182.46591404438 vs Validation_loss  212.00044889576668\n",
      "EPOCHS : 5\n",
      "Loss 6668.312363912762 vs Validation_loss  163.24309845308287\n",
      "EPOCHS : 6\n",
      "Loss 5293.258579738271 vs Validation_loss  149.84749075691258\n",
      "EPOCHS : 7\n",
      "Loss 4883.262364026457 vs Validation_loss  145.89562977731757\n",
      "EPOCHS : 8\n",
      "Loss 4745.866542085335 vs Validation_loss  144.4450462455243\n",
      "EPOCHS : 9\n",
      "Loss 4687.862741957801 vs Validation_loss  143.7610561826588\n",
      "EPOCHS : 10\n",
      "Loss 4658.2901331845205 vs Validation_loss  143.2736669124755\n",
      "EPOCHS : 11\n",
      "Loss 4641.18107641507 vs Validation_loss  142.90056683742895\n",
      "EPOCHS : 12\n",
      "Loss 4630.518343261909 vs Validation_loss  142.5753012958881\n",
      "EPOCHS : 13\n",
      "Loss 4623.504223539382 vs Validation_loss  142.23715270304046\n",
      "EPOCHS : 14\n",
      "Loss 4618.5148328056 vs Validation_loss  141.9508306104525\n",
      "EPOCHS : 15\n",
      "Loss 4614.70569427222 vs Validation_loss  141.7223962882979\n",
      "EPOCHS : 16\n",
      "Loss 4611.6890869174895 vs Validation_loss  141.53446858224615\n",
      "EPOCHS : 17\n",
      "Loss 4609.220086324811 vs Validation_loss  141.3734459655475\n",
      "EPOCHS : 18\n",
      "Loss 4607.135373290479 vs Validation_loss  141.23988913751282\n",
      "EPOCHS : 19\n",
      "Loss 4605.341497423202 vs Validation_loss  141.12865534714894\n",
      "EPOCHS : 20\n",
      "Loss 4603.791051967591 vs Validation_loss  141.0283213727242\n",
      "EPOCHS : 21\n",
      "Loss 4602.41377017349 vs Validation_loss  140.9413528125898\n",
      "EPOCHS : 22\n",
      "Loss 4601.215490174741 vs Validation_loss  140.8512110984431\n",
      "EPOCHS : 23\n",
      "Loss 4600.120277308822 vs Validation_loss  140.75377527169422\n",
      "EPOCHS : 24\n",
      "Loss 4599.133045937121 vs Validation_loss  140.68967532478604\n",
      "EPOCHS : 25\n",
      "Loss 4598.214689111561 vs Validation_loss  140.65772844099365\n",
      "EPOCHS : 26\n",
      "Loss 4597.39235530898 vs Validation_loss  140.61276812574505\n",
      "EPOCHS : 27\n",
      "Loss 4596.656653743982 vs Validation_loss  140.55871162583344\n",
      "EPOCHS : 28\n",
      "Loss 4596.016929164305 vs Validation_loss  140.50491431223608\n",
      "EPOCHS : 29\n",
      "Loss 4595.4708036969605 vs Validation_loss  140.4560479780214\n",
      "EPOCHS : 30\n",
      "Loss 4594.940237982124 vs Validation_loss  140.40027219426315\n",
      "EPOCHS : 31\n",
      "Loss 4594.4212035414575 vs Validation_loss  140.3477676800922\n",
      "EPOCHS : 32\n",
      "Loss 4593.910639499277 vs Validation_loss  140.30096823029814\n",
      "EPOCHS : 33\n",
      "Loss 4593.441921373829 vs Validation_loss  140.25581375793018\n",
      "EPOCHS : 34\n",
      "Loss 4593.011827272102 vs Validation_loss  140.21173360389946\n",
      "EPOCHS : 35\n",
      "Loss 4592.601625567898 vs Validation_loss  140.17874720560766\n",
      "EPOCHS : 36\n",
      "Loss 4592.250234034806 vs Validation_loss  140.14008570350376\n",
      "EPOCHS : 37\n",
      "Loss 4591.9385050437595 vs Validation_loss  140.09574291874878\n",
      "EPOCHS : 38\n",
      "Loss 4591.625996446758 vs Validation_loss  140.05136224126394\n",
      "EPOCHS : 39\n",
      "Loss 4591.30506791152 vs Validation_loss  140.00894864365063\n",
      "EPOCHS : 40\n",
      "Loss 4590.97269866243 vs Validation_loss  139.97719005356848\n",
      "EPOCHS : 41\n",
      "Loss 4590.7731489039215 vs Validation_loss  139.9561808826649\n",
      "EPOCHS : 42\n",
      "Loss 4590.277522921637 vs Validation_loss  139.9436420434344\n",
      "EPOCHS : 43\n",
      "Loss 4589.917944075167 vs Validation_loss  139.91543718143902\n",
      "EPOCHS : 44\n",
      "Loss 4589.602237630859 vs Validation_loss  139.88582894548907\n",
      "EPOCHS : 45\n",
      "Loss 4589.340698079169 vs Validation_loss  139.8548632628095\n",
      "EPOCHS : 46\n",
      "Loss 4589.104178658798 vs Validation_loss  139.82148019507923\n",
      "EPOCHS : 47\n",
      "Loss 4588.871757635772 vs Validation_loss  139.78636372933346\n",
      "EPOCHS : 48\n",
      "Loss 4588.633422264904 vs Validation_loss  139.75211825623975\n",
      "EPOCHS : 49\n",
      "Loss 4588.362446383015 vs Validation_loss  139.71734083648278\n",
      "EPOCHS : 50\n",
      "Loss 4588.071282005385 vs Validation_loss  139.68735518497704\n",
      "EPOCHS : 51\n",
      "Loss 4587.766837906316 vs Validation_loss  139.65603447065945\n",
      "EPOCHS : 52\n",
      "Loss 4587.446325717196 vs Validation_loss  139.62515692584282\n",
      "EPOCHS : 53\n",
      "Loss 4587.110278177708 vs Validation_loss  139.5926447094014\n",
      "EPOCHS : 54\n",
      "Loss 4586.760949605182 vs Validation_loss  139.57399707030407\n",
      "EPOCHS : 55\n",
      "Loss 4586.388113546967 vs Validation_loss  139.56621263723457\n",
      "EPOCHS : 56\n",
      "Loss 4586.017181050405 vs Validation_loss  139.5561456490407\n",
      "EPOCHS : 57\n",
      "Loss 4585.6419550067185 vs Validation_loss  139.5479018899192\n",
      "EPOCHS : 58\n",
      "Loss 4585.271680859625 vs Validation_loss  139.53256403344923\n",
      "EPOCHS : 59\n",
      "Loss 4584.965886517912 vs Validation_loss  139.51314845127342\n",
      "EPOCHS : 60\n",
      "Loss 4584.703359874636 vs Validation_loss  139.48963333653137\n",
      "EPOCHS : 61\n",
      "Loss 4584.43655510202 vs Validation_loss  139.46609663013862\n",
      "EPOCHS : 62\n",
      "Loss 4584.142934461869 vs Validation_loss  139.44188293419052\n",
      "EPOCHS : 63\n",
      "Loss 4583.870654470511 vs Validation_loss  139.41386947484142\n",
      "EPOCHS : 64\n",
      "Loss 4583.58097199399 vs Validation_loss  139.3832560883159\n",
      "EPOCHS : 65\n",
      "Loss 4583.293031776957 vs Validation_loss  139.3642786234881\n",
      "EPOCHS : 66\n",
      "Loss 4583.0164488510045 vs Validation_loss  139.35610546580457\n",
      "EPOCHS : 67\n",
      "Loss 4582.767159140557 vs Validation_loss  139.34360723052404\n",
      "EPOCHS : 68\n",
      "Loss 4582.497824283466 vs Validation_loss  139.32792135783\n",
      "EPOCHS : 69\n",
      "Loss 4582.192051772885 vs Validation_loss  139.31234759672554\n",
      "EPOCHS : 70\n",
      "Loss 4581.86206354808 vs Validation_loss  139.2957738458583\n",
      "EPOCHS : 71\n",
      "Loss 4581.5343555234 vs Validation_loss  139.27908947826486\n",
      "EPOCHS : 72\n",
      "Loss 4581.205863685235 vs Validation_loss  139.2617449222413\n",
      "EPOCHS : 73\n",
      "Loss 4580.887205084078 vs Validation_loss  139.2491475723486\n",
      "EPOCHS : 74\n",
      "Loss 4580.5958065990735 vs Validation_loss  139.23559727394476\n",
      "EPOCHS : 75\n",
      "Loss 4580.281555159613 vs Validation_loss  139.24220519888718\n",
      "EPOCHS : 76\n",
      "Loss 4579.981760356948 vs Validation_loss  139.24726389467187\n",
      "EPOCHS : 77\n",
      "Loss 4579.7670517598835 vs Validation_loss  139.23860405504175\n",
      "EPOCHS : 78\n",
      "Loss 4579.346939374506 vs Validation_loss  139.223057354446\n",
      "EPOCHS : 79\n",
      "Loss 4579.050820167959 vs Validation_loss  139.20930966444774\n",
      "EPOCHS : 80\n",
      "Loss 4578.853798521496 vs Validation_loss  139.19859063308851\n",
      "EPOCHS : 81\n",
      "Loss 4578.6865665178 vs Validation_loss  139.19278478622437\n",
      "EPOCHS : 82\n",
      "Loss 4578.508450575583 vs Validation_loss  139.20131146063846\n",
      "EPOCHS : 83\n",
      "Loss 4578.2231594441455 vs Validation_loss  139.19683898655714\n",
      "EPOCHS : 84\n",
      "Loss 4578.005890947934 vs Validation_loss  139.19200667870783\n",
      "EPOCHS : 85\n",
      "Loss 4577.8308181539 vs Validation_loss  139.1781644525781\n",
      "EPOCHS : 86\n",
      "Loss 4577.651271341294 vs Validation_loss  139.20973176555296\n",
      "EPOCHS : 87\n",
      "Loss 4577.922255951427 vs Validation_loss  139.20602178784597\n",
      "EPOCHS : 88\n",
      "Loss 4577.612515164781 vs Validation_loss  139.1614268480149\n",
      "EPOCHS : 89\n",
      "Loss 4577.3917650454305 vs Validation_loss  139.1299480801135\n",
      "EPOCHS : 90\n",
      "Loss 4577.156513141207 vs Validation_loss  139.11080583006935\n",
      "EPOCHS : 91\n",
      "Loss 4576.95630718533 vs Validation_loss  139.09264560505352\n",
      "EPOCHS : 92\n",
      "Loss 4576.764030243363 vs Validation_loss  139.06947214835512\n",
      "EPOCHS : 93\n",
      "Loss 4576.575380165614 vs Validation_loss  139.0455786000311\n",
      "EPOCHS : 94\n",
      "Loss 4576.394271086548 vs Validation_loss  139.02601061985555\n",
      "EPOCHS : 95\n",
      "Loss 4576.216593245901 vs Validation_loss  139.01166271213935\n",
      "EPOCHS : 96\n",
      "Loss 4576.031302477084 vs Validation_loss  138.9971386405219\n",
      "EPOCHS : 97\n",
      "Loss 4575.8433987227645 vs Validation_loss  138.98628769828156\n",
      "EPOCHS : 98\n",
      "Loss 4575.639772856347 vs Validation_loss  138.97630046004744\n",
      "EPOCHS : 99\n",
      "Loss 4575.432173313107 vs Validation_loss  138.96904584264334\n",
      "EPOCHS : 100\n",
      "Loss 4575.216107876543 vs Validation_loss  138.96126136315607\n",
      "EPOCHS : 101\n",
      "Loss 4574.995223909672 vs Validation_loss  138.9522572935155\n",
      "EPOCHS : 102\n",
      "Loss 4574.771828330662 vs Validation_loss  138.94659840744154\n",
      "EPOCHS : 103\n",
      "Loss 4574.543325720849 vs Validation_loss  138.93950641155243\n",
      "EPOCHS : 104\n",
      "Loss 4574.311930447035 vs Validation_loss  138.93217330696308\n",
      "EPOCHS : 105\n",
      "Loss 4574.085961029772 vs Validation_loss  138.9267898713593\n",
      "EPOCHS : 106\n",
      "Loss 4573.866451775916 vs Validation_loss  138.920494848648\n",
      "EPOCHS : 107\n",
      "Loss 4573.654100010582 vs Validation_loss  138.91694245718222\n",
      "EPOCHS : 108\n",
      "Loss 4573.447844795324 vs Validation_loss  138.91358291258854\n",
      "EPOCHS : 109\n",
      "Loss 4573.248315827958 vs Validation_loss  138.9081445753047\n",
      "EPOCHS : 110\n",
      "Loss 4573.051617266275 vs Validation_loss  138.90325443407076\n",
      "EPOCHS : 111\n",
      "Loss 4572.8547309182395 vs Validation_loss  138.8975662488853\n",
      "EPOCHS : 112\n",
      "Loss 4572.667248327313 vs Validation_loss  138.89209414794382\n",
      "EPOCHS : 113\n",
      "Loss 4572.472841861267 vs Validation_loss  138.8866646796201\n",
      "EPOCHS : 114\n",
      "Loss 4572.284066781485 vs Validation_loss  138.88098814951636\n",
      "EPOCHS : 115\n",
      "Loss 4572.096253589839 vs Validation_loss  138.87578059086758\n",
      "EPOCHS : 116\n",
      "Loss 4571.914057665057 vs Validation_loss  138.86726828275528\n",
      "EPOCHS : 117\n",
      "Loss 4571.72818136652 vs Validation_loss  138.86099047787422\n",
      "EPOCHS : 118\n",
      "Loss 4571.540834114663 vs Validation_loss  138.8532586519697\n",
      "EPOCHS : 119\n",
      "Loss 4571.357791750096 vs Validation_loss  138.8478644052438\n",
      "EPOCHS : 120\n",
      "Loss 4571.17355545382 vs Validation_loss  138.84289554473574\n",
      "EPOCHS : 121\n",
      "Loss 4570.987907550605 vs Validation_loss  138.83685493574734\n",
      "EPOCHS : 122\n",
      "Loss 4570.823409814108 vs Validation_loss  138.82696008576755\n",
      "EPOCHS : 123\n",
      "Loss 4570.67067537101 vs Validation_loss  138.81860122849457\n",
      "EPOCHS : 124\n",
      "Loss 4570.525284315422 vs Validation_loss  138.80973891135866\n",
      "EPOCHS : 125\n",
      "Loss 4570.36841864964 vs Validation_loss  138.8018023007739\n",
      "EPOCHS : 126\n",
      "Loss 4570.207009852846 vs Validation_loss  138.79273774560573\n",
      "EPOCHS : 127\n",
      "Loss 4570.050446179621 vs Validation_loss  138.7841480917635\n",
      "EPOCHS : 128\n",
      "Loss 4569.893108451012 vs Validation_loss  138.77620636467384\n",
      "EPOCHS : 129\n",
      "Loss 4569.735348017141 vs Validation_loss  138.7687801945526\n",
      "EPOCHS : 130\n",
      "Loss 4569.580565958247 vs Validation_loss  138.76058421409235\n",
      "EPOCHS : 131\n",
      "Loss 4569.419239661153 vs Validation_loss  138.75411371319694\n",
      "EPOCHS : 132\n",
      "Loss 4569.25613860894 vs Validation_loss  138.7445531629883\n",
      "EPOCHS : 133\n",
      "Loss 4569.095155410822 vs Validation_loss  138.73757686340704\n",
      "EPOCHS : 134\n",
      "Loss 4568.920944224354 vs Validation_loss  138.72962541073824\n",
      "EPOCHS : 135\n",
      "Loss 4568.749601342678 vs Validation_loss  138.7233510597617\n",
      "EPOCHS : 136\n",
      "Loss 4568.578267399855 vs Validation_loss  138.71474075739363\n",
      "EPOCHS : 137\n",
      "Loss 4568.394576264993 vs Validation_loss  138.70840588502125\n",
      "EPOCHS : 138\n",
      "Loss 4568.215729301646 vs Validation_loss  138.702099086964\n",
      "EPOCHS : 139\n",
      "Loss 4568.030692195557 vs Validation_loss  138.69715370541124\n",
      "EPOCHS : 140\n",
      "Loss 4567.844430622049 vs Validation_loss  138.69166897144993\n",
      "EPOCHS : 141\n",
      "Loss 4567.660942578018 vs Validation_loss  138.68725066480383\n",
      "EPOCHS : 142\n",
      "Loss 4567.4790307058765 vs Validation_loss  138.68313533225947\n",
      "EPOCHS : 143\n",
      "Loss 4567.2945577804 vs Validation_loss  138.6803897608698\n",
      "EPOCHS : 144\n",
      "Loss 4567.123524053693 vs Validation_loss  138.67882302693562\n",
      "EPOCHS : 145\n",
      "Loss 4566.957407626361 vs Validation_loss  138.67449906336523\n",
      "EPOCHS : 146\n",
      "Loss 4566.796600849368 vs Validation_loss  138.67051358771536\n",
      "EPOCHS : 147\n",
      "Loss 4566.626438403204 vs Validation_loss  138.6658247213448\n",
      "EPOCHS : 148\n",
      "Loss 4566.447927512936 vs Validation_loss  138.66500064229544\n",
      "EPOCHS : 149\n",
      "Loss 4566.272112089135 vs Validation_loss  138.6677319201748\n",
      "EPOCHS : 150\n",
      "Loss 4566.09984894827 vs Validation_loss  138.67055362304754\n",
      "EPOCHS : 151\n",
      "Loss 4565.939367011152 vs Validation_loss  138.67172810778155\n",
      "EPOCHS : 152\n",
      "Loss 4565.787533371002 vs Validation_loss  138.67226901413065\n",
      "EPOCHS : 153\n",
      "Loss 4565.643474991917 vs Validation_loss  138.6666801028547\n",
      "EPOCHS : 154\n",
      "Loss 4565.495466863774 vs Validation_loss  138.66346784397564\n",
      "EPOCHS : 155\n",
      "Loss 4565.30897600323 vs Validation_loss  138.66030726179613\n",
      "EPOCHS : 156\n",
      "Loss 4565.159175110906 vs Validation_loss  138.65868447839685\n",
      "EPOCHS : 157\n",
      "Loss 4565.04596334774 vs Validation_loss  138.6615308797465\n",
      "EPOCHS : 158\n",
      "Loss 4564.92616530478 vs Validation_loss  138.66482810425546\n",
      "EPOCHS : 159\n",
      "Loss 4564.790023275093 vs Validation_loss  138.6654774699591\n",
      "EPOCHS : 160\n",
      "Loss 4564.647927950547 vs Validation_loss  138.66546204765285\n",
      "EPOCHS : 161\n",
      "Loss 4564.507274106145 vs Validation_loss  138.66341919181622\n",
      "EPOCHS : 162\n",
      "Loss 4564.3555343077705 vs Validation_loss  138.66383510036806\n",
      "EPOCHS : 163\n",
      "Loss 4564.192249321789 vs Validation_loss  138.66758602066378\n",
      "EPOCHS : 164\n",
      "Loss 4564.031075605936 vs Validation_loss  138.6718657122249\n",
      "EPOCHS : 165\n",
      "Loss 4563.863322696798 vs Validation_loss  138.67496329176743\n",
      "EPOCHS : 166\n",
      "Loss 4563.694376978204 vs Validation_loss  138.6805031700472\n",
      "EPOCHS : 167\n",
      "Loss 4563.520951454751 vs Validation_loss  138.68605857507316\n",
      "EPOCHS : 168\n",
      "Loss 4563.336618044675 vs Validation_loss  138.6911701132766\n",
      "EPOCHS : 169\n",
      "Loss 4563.143447274566 vs Validation_loss  138.6929817948721\n",
      "EPOCHS : 170\n",
      "Loss 4562.966951115131 vs Validation_loss  138.68833581008744\n",
      "EPOCHS : 171\n",
      "Loss 4562.806413989477 vs Validation_loss  138.6891966129826\n",
      "EPOCHS : 172\n",
      "Loss 4562.641998854 vs Validation_loss  138.69175121742012\n",
      "EPOCHS : 173\n",
      "Loss 4562.480830717087 vs Validation_loss  138.69243747998127\n",
      "EPOCHS : 174\n",
      "Loss 4562.328959307354 vs Validation_loss  138.69577824752943\n",
      "EPOCHS : 175\n",
      "Loss 4562.166296631433 vs Validation_loss  138.70020486401245\n",
      "EPOCHS : 176\n",
      "Loss 4562.0098953308725 vs Validation_loss  138.70530062544663\n",
      "EPOCHS : 177\n",
      "Loss 4561.853974404018 vs Validation_loss  138.71050504562075\n",
      "EPOCHS : 178\n",
      "Loss 4561.700001227129 vs Validation_loss  138.7158259427653\n",
      "EPOCHS : 179\n",
      "Loss 4561.539055762086 vs Validation_loss  138.72281502833408\n",
      "EPOCHS : 180\n",
      "Loss 4561.376556980182 vs Validation_loss  138.7279660153178\n",
      "EPOCHS : 181\n",
      "Loss 4561.216668423544 vs Validation_loss  138.73224189429158\n",
      "EPOCHS : 182\n",
      "Loss 4561.05693595063 vs Validation_loss  138.7367161069296\n",
      "EPOCHS : 183\n",
      "Loss 4560.895787452106 vs Validation_loss  138.73881478014246\n",
      "EPOCHS : 184\n",
      "Loss 4560.740716762505 vs Validation_loss  138.74119715669514\n",
      "EPOCHS : 185\n",
      "Loss 4560.590468775779 vs Validation_loss  138.74435422378303\n",
      "EPOCHS : 186\n",
      "Loss 4560.4380428766835 vs Validation_loss  138.74518198460603\n",
      "EPOCHS : 187\n",
      "Loss 4560.288386650867 vs Validation_loss  138.74536577368204\n",
      "EPOCHS : 188\n",
      "Loss 4560.145378809049 vs Validation_loss  138.74342225083208\n",
      "EPOCHS : 189\n",
      "Loss 4560.0150157995895 vs Validation_loss  138.73993447291113\n",
      "EPOCHS : 190\n",
      "Loss 4559.894429199099 vs Validation_loss  138.7342925166662\n",
      "EPOCHS : 191\n",
      "Loss 4559.769804819338 vs Validation_loss  138.73017999661707\n",
      "EPOCHS : 192\n",
      "Loss 4559.6476878678795 vs Validation_loss  138.72550035472466\n",
      "EPOCHS : 193\n",
      "Loss 4559.522282752953 vs Validation_loss  138.72213462074245\n",
      "EPOCHS : 194\n",
      "Loss 4559.390341326818 vs Validation_loss  138.7192223367438\n",
      "EPOCHS : 195\n",
      "Loss 4559.260071153157 vs Validation_loss  138.71696252421995\n",
      "EPOCHS : 196\n",
      "Loss 4559.128831344098 vs Validation_loss  138.7149930274592\n",
      "EPOCHS : 197\n",
      "Loss 4558.9868132327865 vs Validation_loss  138.71189311647836\n",
      "EPOCHS : 198\n",
      "Loss 4558.857412685082 vs Validation_loss  138.70933950052853\n",
      "EPOCHS : 199\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058783-9d81-4832-831f-5df05b70daf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
